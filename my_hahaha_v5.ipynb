{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import _pickle as cPickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPool1D, merge, Dropout, LSTM, GRU, Bidirectional, TimeDistributed, Dense, Input, Flatten, CuDNNGRU,CuDNNLSTM, concatenate, Lambda \n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "from Attention import AttentionLayer\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 100\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT,TEST_SPLIT = 0.2 , 0.05\n",
    "DROP_RATE=0.45\n",
    "ATTENTION_TYPE='local' #{'self','global','local'}\n",
    "GPU=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the text, tokenization and creating training, validation and test tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    string = re.sub(r\"\\\\\", \"\", string)\n",
    "    string = re.sub(r\"\\'\", \"\", string)\n",
    "    string = re.sub(r\"\\\"\", \"\", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "data_train = pd.read_csv('labeledTrainData.tsv', sep='\\t')\n",
    "\n",
    "from nltk import tokenize\n",
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for idx in range(data_train.review.shape[0]):\n",
    "    text = BeautifulSoup(data_train.review[idx])\n",
    "    text = clean_text(text.get_text())\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    reviews.append(sentences)\n",
    "\n",
    "    labels.append(data_train.sentiment[idx])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "\n",
    "# creating and filling the 3D tensor with word_index numbers:\n",
    "\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('X tensor shape:', data.shape)\n",
    "print('Y tensor shape:', labels.shape)\n",
    "\n",
    "\n",
    "# suffle data and split train, validation and test sets:\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "nb_val = int(VALIDATION_SPLIT* data.shape[0])\n",
    "nb_test = int(TEST_SPLIT* data.shape[0])\n",
    "\n",
    "x_train = data[:-(nb_val+nb_test)]\n",
    "y_train = labels[:-(nb_val+nb_test)]\n",
    "x_val = data[-(nb_val+nb_test):-nb_test]\n",
    "y_val = labels[-(nb_val+nb_test):-nb_test]\n",
    "x_test = data[-nb_test:]\n",
    "y_test = labels[-nb_test:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the glove.txt to dictionary: \n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('glove.6B.100d.txt'),encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "\n",
    "# using the embedding_index dictionary and word_index to create embeding_matrix:\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating hidden layers, applying attention to sentence and document levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SENTENCE LEVEL\n",
    "sent_ints = Input(shape=(None,))\n",
    "sent_wv = Embedding(embedding_matrix.shape[0],\n",
    "                    embedding_matrix.shape[1],\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=MAX_SENT_LENGTH, \n",
    "                    trainable=False\n",
    "                    )(sent_ints)\n",
    "\n",
    "sent_wv_dr = Dropout(DROP_RATE)(sent_wv)\n",
    "\n",
    "if GPU : sent_wa = Bidirectional(CuDNNGRU(units=50,return_sequences=True),merge_mode='concat',weights=None)(sent_wv_dr)\n",
    "else : sent_wa = Bidirectional(GRU(units=50,return_sequences=True),merge_mode='concat',weights=None)(sent_wv_dr)\n",
    "\n",
    "# attention vector for the sentence:\n",
    "sent_att_vec,sent_att_coeffs = AttentionLayer(return_coefficients=True, attention_type=ATTENTION_TYPE)(sent_wa) \n",
    "sent_att_vec_dr = Dropout(DROP_RATE)(sent_att_vec)                      \n",
    "sent_encoder = Model(sent_ints,sent_att_vec_dr)\n",
    "\n",
    "print(np.shape(sent_encoder))\n",
    "print(sent_wa.shape)\n",
    "print(sent_att_vec_dr.shape)\n",
    "\n",
    "\n",
    "#DOCUMENT LEVEL\n",
    "doc_ints = Input(shape=(None,None,))        \n",
    "sent_att_vecs_dr = TimeDistributed(sent_encoder)(doc_ints)\n",
    "\n",
    "if GPU : doc_sa = Bidirectional(CuDNNGRU(units=50,return_sequences=True),merge_mode='concat',weights=None)(sent_att_vecs_dr)\n",
    "else : doc_sa = Bidirectional(GRU(units=50,return_sequences=True),merge_mode='concat',weights=None)(sent_att_vecs_dr)\n",
    "\n",
    "# attention vector for the document:\n",
    "doc_att_vec,doc_att_coeffs = AttentionLayer(return_coefficients=True, attention_type=ATTENTION_TYPE)(doc_sa)\n",
    "doc_att_vec_dr = Dropout(DROP_RATE)(doc_att_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the output layer and assembling the NN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cats=np.shape(y_train)[1]\n",
    "preds = Dense(units=n_cats, activation='softmax')(doc_att_vec_dr)\n",
    "\n",
    "Classifier = Model(doc_ints,preds)\n",
    "\n",
    "Classifier.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = Classifier.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=20, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots and metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "train_loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epcs= range (1, len(train_acc)+1)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "\n",
    "ax1.plot(epcs, train_acc, color='b',label=\"Training\")\n",
    "ax1.plot(epcs, val_acc, color='r',label=\"Validation\")\n",
    "\n",
    "ax1.set_facecolor('lightyellow')\n",
    "ax1.grid(True)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_facecolor('lightcyan')\n",
    "ax2.plot(epcs, train_loss, color='b',label=\"Ttraining\")\n",
    "ax2.plot(epcs, val_loss, color='r',label=\"Validation\")\n",
    "ax2.grid(True)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Classifier.predict(x_test)\n",
    "\n",
    "def transfer2cat (m):\n",
    "    mm=[]\n",
    "    for i in m.tolist():\n",
    "        mm.append(i.index(max(i))+1)\n",
    "    return mm\n",
    "\n",
    "prediction=transfer2cat(prediction)\n",
    "y_test=transfer2cat(np.array(y_test))\n",
    "\n",
    "\n",
    "print ('Accuracy:', accuracy_score(y_test, prediction))\n",
    "print ('F1 score:', f1_score(y_test, prediction, average='weighted'))\n",
    "print ('Recall:', recall_score(y_test, prediction, average='weighted'))\n",
    "print ('Precision:', precision_score(y_test, prediction, average='weighted'))\n",
    "print ('\\n clasification report:\\n', classification_report(y_test,prediction))\n",
    "print ('\\n confussion matrix:\\n',confusion_matrix(y_test, prediction))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
